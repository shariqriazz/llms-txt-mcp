# --- Tavily API Configuration ---
# Required for topic discovery in llms-full_crawl
TAVILY_API_KEY=YOUR_TAVILY_API_KEY_HERE

# --- LLM Configuration ---
# Specifies the provider for the LLMS-Full pipeline synthesis stage. Choose 'gemini' (default), 'ollama', 'openrouter', 'groq', or 'chutes'.
PIPELINE_LLM_PROVIDER=gemini
# The specific model name for the chosen PIPELINE_LLM_PROVIDER.
# Defaults depend on provider ('gemini-2.0-flash', 'llama3.1:8b', 'openai/gpt-3.5-turbo').
PIPELINE_LLM_MODEL=gemini-2.0-flash

# Specifies the provider for the Synthesize Answer tool. Choose 'gemini' (default), 'ollama', or 'openrouter'.
SYNTHESIZE_LLM_PROVIDER=gemini
# The specific model name for the chosen SYNTHESIZE_LLM_PROVIDER.
# Defaults depend on provider ('gemini-2.0-flash', 'llama3.1:8b', 'openai/gpt-3.5-turbo').
SYNTHESIZE_LLM_MODEL=gemini-2.0-flash

# --- LLM Provider Credentials ---
# Required if *any* LLM provider is 'gemini' (or embedding provider is 'google'). Get from Google AI Studio.
GEMINI_API_KEY=YOUR_GOOGLE_AI_STUDIO_KEY_HERE
# Required if *any* LLM provider is 'openrouter'. Get from OpenRouter.ai.
OPENROUTER_API_KEY=YOUR_OPENROUTER_API_KEY_HERE
# Required if *any* LLM provider is 'chutes'. Get from Chutes.
CHUTES_API_KEY=YOUR_CHUTES_API_KEY_HERE
# Optional: Base URL for Ollama if *any* LLM provider is 'ollama' and it's not at the default location.
# Example: http://localhost:11434 or http://host.docker.internal:11434
# OLLAMA_BASE_URL=
# Optional: Base URL for OpenRouter API if not default and *any* LLM provider is 'openrouter'.
# OPENROUTER_BASE_URL=
# Optional: Base URL for Chutes API (defaults to https://llm.chutes.ai/v1).
# CHUTES_BASE_URL=

# --- llms-full Qdrant Configuration ---
# URL of your Qdrant instance. Required for all llms-full tools.
QDRANT_URL=http://localhost:6333
# Optional: API key for Qdrant Cloud or secured instances.
# QDRANT_API_KEY=YOUR_QDRANT_API_KEY_HERE

# --- llms-full Embedding Configuration ---
# Specifies the embedding model provider. Choose one: 'openai', 'ollama', 'google'. Required for llms-full indexing/search.
EMBEDDING_PROVIDER=ollama

# --- OpenAI Embedding Settings (if EMBEDDING_PROVIDER=openai) ---
# Required if EMBEDDING_PROVIDER is 'openai'.
# OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
# Optional: Base URL for OpenAI-compatible APIs (e.g., Together.ai, Anyscale).
# OPENAI_BASE_URL=

# --- Ollama Embedding Settings (if EMBEDDING_PROVIDER=ollama) ---
# Required if EMBEDDING_PROVIDER is 'ollama'. The model name to use for embeddings.
# Example: nomic-embed-text, mxbai-embed-large
OLLAMA_MODEL=nomic-embed-text
# Note: Ollama connection URL is typically handled by the OLLAMA_HOST env var used by the 'ollama' library, or defaults to http://localhost:11434.
# You might set OLLAMA_HOST separately if needed globally for the ollama library.

# --- Google (Gemini) Embedding Settings (if EMBEDDING_PROVIDER=google) ---
# Required if EMBEDDING_PROVIDER is 'google'. Uses the same GEMINI_API_KEY as LLM processing.
# Optional: Specific Gemini embedding model name (defaults to models/embedding-001).
# EMBEDDING_MODEL=models/embedding-001
# Optional: A fallback Gemini embedding model name if the primary fails.
# GEMINI_FALLBACK_MODEL=text-embedding-004